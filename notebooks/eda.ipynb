{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\program files\\python38\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: joblib in c:\\program files\\python38\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\program files\\python38\\lib\\site-packages (from nltk) (8.1.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\program files\\python38\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\program files\\python38\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python38\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import string\n",
    "from nltk.probability import FreqDist\n",
    "import statistics\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters in dataset: 192761441\n"
     ]
    }
   ],
   "source": [
    "# Path should be changed according to the user's project location\n",
    "path = Path(\"C:/Users/User/Desktop/NLP/Final-Project/NLP-Final-Project/data/kawiki.txt\")\n",
    "\n",
    "# Reading the file contents\n",
    "file = open(path, \"r\", encoding=\"utf8\")\n",
    "text = file.read()\n",
    "\n",
    "print (\"Total characters in dataset: {}\".format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 500 characters: \n",
      "ედუარდ შევარდნაძე  ედუარდ ამბროსის ძე შევარდნაძე დ. 25 იანვარი 1928 სოფელი მამათი ოზურგეთის მაზრა საქართველოს სსრ  გ. 7 ივლისი 2014 თბილისი საქართველო  ქართველი პოლიტიკოსი და სახელმწიფო მოღვაწე. 19721985 წლებში საქართველოს სსრის კომპარტიის ცკს პირველი მდივანი 19851990 წლებში საბჭოთა კავშირის საგარეო საქმეთა მინისტრი 19952003 წლებში საქართველოს პრეზიდენტი. იყო სსრკის IXXI მოწვევების უმაღლესი საბჭოს დეპუტატი. სოციალისტური შრომის გმირი 1981 სკკპ ცკის პოლიტბიუროს წევრობის კანდიდატი 1978 წევრი 1985 წ\n"
     ]
    }
   ],
   "source": [
    "# We are not removing periods in order to keep a sentence count\n",
    "string.punctuation = string.punctuation +'“'+'”'+'’'+'‘'+'-'+'—'+'„'+'–'\n",
    "string.punctuation = string.punctuation.replace('.', '')\n",
    "\n",
    "file = open(path, encoding=\"utf8\").read()\n",
    "\n",
    "# Preprocess data to remove new lines and special characters\n",
    "file_new = \"\"\n",
    "index = 0\n",
    "\n",
    "for line in file:\n",
    "    line_new = line.replace(\"\\n\", \" \")\n",
    "    file_new += line_new\n",
    "    index += 1\n",
    "    \n",
    "    if index > 4000000:\n",
    "        break\n",
    "    \n",
    "processed_corpus = \"\".join([char for char in file_new if char not in string.punctuation])\n",
    "\n",
    "print(\"First 500 characters: \")\n",
    "print(processed_corpus[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizes the text into arrays of sentences and seperate words\n",
    "sentences = sent_tokenize(processed_corpus)\n",
    "words = word_tokenize(processed_corpus)\n",
    "words = [word for word in words if word != '.']\n",
    "\n",
    "print(\"First five sentences: \")\n",
    "print(sentences[0:5])\n",
    "print(\"First five words: \")\n",
    "print(words[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average word length\n",
    "lengths = [len(word) for word in words]\n",
    "average = sum(lengths) / len(lengths)\n",
    "print(\"Average word length: \" + str(average))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean word length\n",
    "median = statistics.median(lengths)\n",
    "print(\"Median word length: \" + str(median))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average sentence length\n",
    "sentence_lengths = [len(sentence) for sentence in sentences]\n",
    "average_sentence = sum(sentence_lengths) / len(sentence_lengths)\n",
    "print(\"Average sentence length: \" + str(average_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean sentence length\n",
    "median_sentence = statistics.median(sentence_lengths)\n",
    "print(\"Median sentence length: \" + str(median_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most frequent words\n",
    "def get_most_frequent(words, k):\n",
    "      word_frequency = {}\n",
    "      frequency_list = {}\n",
    "    \n",
    "      for i in words:\n",
    "         if i not in word_frequency:\n",
    "            word_frequency[i] = 1\n",
    "         else:\n",
    "            word_frequency[i] += 1\n",
    "            \n",
    "      for key, value in word_frequency.items():\n",
    "         if value not in frequency_list:\n",
    "            frequency_list[value] = [key]\n",
    "         else:\n",
    "            frequency_list[value].append(key)\n",
    "            \n",
    "      result = []\n",
    "    \n",
    "      for i in range(len(words), 0, -1):\n",
    "         if i in frequency_list:\n",
    "            result.extend(frequency_list[i])\n",
    "         if len(result) >= k:\n",
    "            break\n",
    "            \n",
    "      return result\n",
    "\n",
    "most_frequent = get_most_frequent(words, 20)\n",
    "print(\"Top 20 most frequent words: \" + str(most_frequent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most frequent words without stop words\n",
    "stop_words = open(\"C:/Users/User/Desktop/NLP/Final-Project/NLP-Final-Project/data/stop_words.txt\", 'r', encoding='utf8').read().splitlines()\n",
    "no_stop_words = [word for word in words if word not in stop_words]\n",
    "most_frequent_no_stop = get_most_frequent(no_stop_words, 20)\n",
    "\n",
    "print(\"Top 20 most frequent words excluding stop words: \" + str(most_frequent_no_stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
